import os
import sys
import subprocess
import platform
import autogen
from typing import Dict
from dotenv import load_dotenv

load_dotenv()

# Read strict specification
spec_file_path = "docs/specs.txt"
spec_content = ""
if os.path.exists(spec_file_path):
    with open(spec_file_path, "r", encoding="utf-8") as f:
        spec_content = f.read()
else:
    print(f"Warning: {spec_file_path} not found.")

def check_and_pull_ollama_model(model_name: str = "llama3:8b"):
    """Checks if the Ollama model exists, pulls it if not."""

    # OS-Specific Path Setup
    current_os = platform.system()
    if current_os == "Windows":
        # Dynamic path to Local AppData
        local_app_data = os.environ.get("LOCALAPPDATA")
        if local_app_data:
            ollama_path = os.path.join(local_app_data, "Programs", "Ollama")
            # Add to PATH if it exists and not already there
            if os.path.exists(ollama_path) and ollama_path not in os.environ["PATH"]:
                print(f"Adding {ollama_path} to PATH...")
                os.environ["PATH"] += os.pathsep + ollama_path

    try:
        # Check if model exists
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        if model_name not in result.stdout:
            print(f"Model {model_name} not found. Pulling...")
            subprocess.run(["ollama", "pull", model_name], check=True)
            print(f"Model {model_name} pulled successfully.")
        else:
            print(f"Model {model_name} found.")
    except FileNotFoundError:
        print(
            "Error: 'ollama' command not found. Please ensure Ollama is installed and in your PATH."
        )
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"Error pulling model: {e}")
        sys.exit(1)


def get_llm_config() -> Dict:
    """Returns the LLM configuration based on environment variables."""
    api_key = os.environ.get("GEMINI_API_KEY")

    if api_key:
        print("Using Gemini 2.5 Flash configuration.")
        return {
            "config_list": [
                {"model": "gemini-2.5-flash", "api_key": api_key, "api_type": "google"}
            ],
            "temperature": 0.2,  # Lower temperature for coding/TDD
            "seed": 42,
        }
    else:
        print("GEMINI_API_KEY not found. Falling back to local Ollama.")
        model = "llama3:8b"
        check_and_pull_ollama_model(model)
        return {
            "config_list": [
                {
                    "model": model,
                    "base_url": "http://localhost:11434/v1",
                    "api_key": "ollama",  # Required placeholder
                }
            ],
            "temperature": 0.2,
            "seed": 42,
            "request_timeout": 300,  # Increased timeout for local execution
        }


llm_config = get_llm_config()


# --- Custom Agent Classes ---

class CleanUserProxyAgent(autogen.UserProxyAgent):
    """A UserProxyAgent with a cleaner UI prompt."""
    def get_human_input(self, prompt: str) -> str:
        # Ignore the verbose default prompt
        custom_prompt = (
            "\n"
            "   [ENTER] Run Code / Approve\n"
            "   [Type]  Give Instructions\n"
            "   [exit]  Quit\n"
            "> "
        )
        return input(custom_prompt)

# --- Agent Definitions ---

# --- Agent Definitions ---

# IaC_Architect: Planner and QA.
iac_architect = autogen.AssistantAgent(
    name="IaC_Architect",
    system_message="""You are an expert Network Automation Architect and QA lead.
    Your goal is to ensure the IaC Workbook is technically accurate and follows the 'DETAILED SPECIFICATION'.
    1. Guide the 'Workbook_Editor' on the structure of the next chapter/section.
    2. Review the content generated by 'Workbook_Editor' for technical correctness (Idempotency, IaC best practices).
    3. Ensure logical flow (e.g., Variable Precedence is explained correctly).
    4. If revisions are needed, instruct 'Workbook_Editor' to fix them.
    5. Hand off to 'Workbook_Editor' to write the content.
    Ends your message by explicitly naming the next speaker: "Workbook_Editor, please proceed with [Section Name]."
    When instructing the next speaker, consider the following document as the golden source, pick the parts that are relevant for the task:
    """ + spec_content,
    llm_config=llm_config,
)

# Workbook_Editor: Content Writer.
workbook_editor = autogen.AssistantAgent(
    name="Workbook_Editor",
    system_message="""You are the Lead Technical Writer and Editor for the IaC Workbook.
    Your goal is to write clear, comprehensive Markdown content based on the guidance received from the other agents.
    1. Write the narrative text, explanations, and tables based on the Architect's guidance and the Spec.
    2. SAVE the content to a file (e.g., `docs/workbook_part1.md`) by outputting a PYTHON BLOCK that writes the file.
       
       CRITICAL: Your content will contain Markdown code blocks (```) and quotes. 
       To avoid Python SyntaxErrors, you MUST use `repr()` or string concatenation, or carefully escape quotes.
       
       SAFE PATTERN Example:
       ```python
       content = (
           "# Title\\n"
           "Some text with \\"quotes\\" and code:\\n"
           "```python\\nprint('hello')\\n```\\n"
       )
       import os
       os.makedirs('docs', exist_ok=True)
       with open('docs/workbook_part1.md', 'a', encoding='utf-8') as f:
           f.write(content)
       ```
    3. Use 'a' (append) mode if adding to an existing file chapter.
    4. Call the 'IaC_Coder' to generate specific code snippets (Python, Terraform, Ansible) to be saved in `src/`.
    5. Ends your message by explicitly naming the next speaker: 
       - "Executor, please save this file. After saving, the next speaker is IaC_Coder to provide examples."
       - "Executor, please save this file. After saving, the next speaker is Reviewer."
    """,
    llm_config=llm_config,
)

# IaC_Coder: Code Generator.
iac_coder = autogen.AssistantAgent(
    name="IaC_Coder",
    system_message="""You are a Senior DevOps Engineer and Polyglot Coder.
    Your goal is to generate high-quality code examples for the Workbook.
    1. Listen to requests from 'Workbook_Editor'.
    2. Generate specific code snippets (Pydantic models, Terraform HCL, Ansible YAML, etc.).
    3. SAVE the code to the requested file path by outputting a PYTHON BLOCK that writes the file.
       CRITICAL: Handle quotes carefully to avoid SyntaxErrors.
       
       Example:
       ```python
       code_content = (
           "resource \\"aws_s3_bucket\\" \\"b\\" {\\n"
           "  bucket = \\"my-bucket\\"\\n"
           "}\\n"
       )
       import os
       os.makedirs('src/terraform', exist_ok=True)
       with open('src/terraform/main.tf', 'w', encoding='utf-8') as f:
           f.write(code_content)
       ```
    4. ALWAYS wrap code in valid Markdown code blocks for display, AND provide the Python block to save it.
    5. Hand off back to 'Workbook_Editor' to continue the document flow.
    Ends your message by explicitly naming the next speaker: "Executor, please save this code. After saving, the next speaker is Workbook_Editor."
    """,
    llm_config=llm_config,
)

# Reviewer: Style and Format Checker.
reviewer = autogen.AssistantAgent(
    name="Reviewer",
    system_message="""You are a strict Technical Editor and Proofreader.
    Your goal is to polish the final output.
    1. Check for Markdown syntax errors (broken tables, unclosed blocks).
    2. Check for grammatical issues and clarity.
    3. Verify that code blocks provided by 'IaC_Coder' are formatted correctly.
    4. If issues found, instruct 'Workbook_Editor' to fix.
    5. If the section is perfect, confirm completion to the Architect or proceed to the next topic.
    Ends your message by explicitly naming the next speaker:
    - "Workbook_Editor, please fix [Issue]."
    - "IaC_Architect, Section complete. Ready for next topic."
    When proofreading, consider the following document as the golden source:
    """ + spec_content,
    llm_config=llm_config,
)

# Executor: Runs commands (Kept from original, but mainly for file I/O if needed later).
executor = CleanUserProxyAgent(
    name="Executor",
    system_message="""You are the build server and execution environment.
    1. You execute python scripts, pytest commands, and linter checks if requested.
    2. Working directory: '.', but code is in 'src/' and 'tests/'.
    3. When ANY agent provides code blocks (bash or python), execute them if instructed.
    4. Report the stdout and stderr back to the chat.
    5. Be ready to accept "Human Input" at any stage (`human_input_mode="ALWAYS"`).
    """,
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "work_dir": ".",
        "use_docker": False,
    },
)

# --- Group Chat Flow ---

groupchat = autogen.GroupChat(
    agents=[iac_architect, workbook_editor, iac_coder, reviewer, executor],
    messages=[],
    max_round=100,
)

# Strict sequential flow managed by the system messages and speaker selection
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)

# --- Start ---

# --- Start ---



initial_task = (
    "Kezd meg a NetDevOps IaC Workbook Library létrehozását, szigorúan a "
    "lenti 'DETAILED SPECIFICATION' dokumentum fejezeteinek tartalmával, "
    "Markdown formátumban. A Git Workflow és a DevContainers bevezetésével kezdj!\n\n"
    "--- DETAILED SPECIFICATION ---\n"
    f"{spec_content}"
)

print("Starting IaC Workbook Agents...")
executor.initiate_chat(manager, message=initial_task)
