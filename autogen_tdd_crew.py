import os
import sys
import subprocess
import platform
import autogen
from typing import Dict
from dotenv import load_dotenv

load_dotenv()

# Read strict specification
spec_file_path = "docs/specs.txt"
spec_content = ""
if os.path.exists(spec_file_path):
    with open(spec_file_path, "r", encoding="utf-8") as f:
        spec_content = f.read()
else:
    print(f"Warning: {spec_file_path} not found.")

def check_and_pull_ollama_model(model_name: str = "llama3:8b"):
    """Checks if the Ollama model exists, pulls it if not."""

    # OS-Specific Path Setup
    current_os = platform.system()
    if current_os == "Windows":
        # Dynamic path to Local AppData
        local_app_data = os.environ.get("LOCALAPPDATA")
        if local_app_data:
            ollama_path = os.path.join(local_app_data, "Programs", "Ollama")
            # Add to PATH if it exists and not already there
            if os.path.exists(ollama_path) and ollama_path not in os.environ["PATH"]:
                print(f"Adding {ollama_path} to PATH...")
                os.environ["PATH"] += os.pathsep + ollama_path

    try:
        # Check if model exists
        result = subprocess.run(["ollama", "list"], capture_output=True, text=True)
        if model_name not in result.stdout:
            print(f"Model {model_name} not found. Pulling...")
            subprocess.run(["ollama", "pull", model_name], check=True)
            print(f"Model {model_name} pulled successfully.")
        else:
            print(f"Model {model_name} found.")
    except FileNotFoundError:
        print(
            "Error: 'ollama' command not found. Please ensure Ollama is installed and in your PATH."
        )
        sys.exit(1)
    except subprocess.CalledProcessError as e:
        print(f"Error pulling model: {e}")
        sys.exit(1)


def get_llm_config() -> Dict:
    """Returns the LLM configuration based on environment variables."""
    api_key = os.environ.get("GEMINI_API_KEY")

    if api_key:
        print("Using Gemini 2.5 Flash configuration.")
        return {
            "config_list": [
                {"model": "gemini-2.5-flash", "api_key": api_key, "api_type": "google"}
            ],
            "temperature": 0.2,  # Lower temperature for coding/TDD
        }
    else:
        print("GEMINI_API_KEY not found. Falling back to local Ollama.")
        model = "llama3:8b"
        check_and_pull_ollama_model(model)
        return {
            "config_list": [
                {
                    "model": model,
                    "base_url": "http://localhost:11434/v1",
                    "api_key": "ollama",  # Required placeholder
                }
            ],
            "temperature": 0.2,
            "request_timeout": 300,  # Increased timeout for local execution
        }


llm_config = get_llm_config()


# --- Custom Agent Classes ---

class CleanUserProxyAgent(autogen.UserProxyAgent):
    """A UserProxyAgent with a cleaner UI prompt."""
    def get_human_input(self, prompt: str) -> str:
        # Ignore the verbose default prompt
        custom_prompt = (
            "\n"
            "   [ENTER] Run Code / Approve\n"
            "   [Type]  Give Instructions\n"
            "   [exit]  Quit\n"
            "> "
        )
        return input(custom_prompt)

# --- Agent Definitions ---

# --- Agent Definitions ---

# IaC_Architect: Planner and QA.
iac_architect = autogen.AssistantAgent(
    name="IaC_Architect",
    system_message="""You are an expert Network Automation Architect and QA lead.
    Your goal is to ensure the IaC Workbook is technically accurate and follows the 'DETAILED SPECIFICATION'.
    1. Guide the 'Workbook_Editor' on the structure of the next chapter/section.
    2. Review the content generated by 'Workbook_Editor' for technical correctness (Idempotency, IaC best practices).
    3. Ensure logical flow (e.g., Variable Precedence is explained correctly).
    4. If revisions are needed, instruct 'Workbook_Editor' to fix them.
    5. Hand off to 'Workbook_Editor' to write the content.
    Ends your message by explicitly naming the next speaker: "Workbook_Editor, please proceed with [Section Name]."
    When instructing the next speaker, consider the following document as the golden source, pick the parts that are relevant for the task:
    """ + spec_content,
    llm_config=llm_config,
)

# Workbook_Editor: Content Writer.
workbook_editor = autogen.AssistantAgent(
    name="Workbook_Editor",
    system_message="""You are the Lead Technical Writer and Editor.
    Your goal is to write clear, comprehensive Markdown content based on the Guidance.
    1. Write the narrative text, explanations, and tables.
    2. SAVE the content to a specific file for that chapter.
       - Use filenames like `docs/chapter_01_git.md`, `docs/chapter_02_devcontainers.md`, etc.
       - DO NOT use generic names like "part1" to avoid overwriting.
       
       CRITICAL: Use safe string handling for content writing.
       ```python
       content = (
           "# Chapter X\\n"
           "Content...\\n"
       )
       import os
       os.makedirs('docs', exist_ok=True)
       with open('docs/chapter_0X_topic.md', 'w', encoding='utf-8') as f:
           f.write(content)
       ```
    3. Call the 'IaC_Coder' to generate specific code snippets if needed.
    4. If no code is needed, proceed to the next chapter yourself or ask Architect for the next topic.
    
    Ends your message by explicitly naming the next speaker: 
    - "Executor, please save this file. After saving, the next speaker is IaC_Coder."
    - "Executor, please save this file. After saving, the next speaker is IaC_Architect."
    """,
    llm_config=llm_config,
)

# IaC_Coder: Code Generator.
iac_coder = autogen.AssistantAgent(
    name="IaC_Coder",
    system_message="""You are a Senior DevOps Engineer.
    1. Generate specific code snippets (Pydantic models, Terraform HCL, Ansible YAML).
    2. SAVE the code to `src/` with appropriate filenames.
       CRITICAL: Handle quotes carefully.
       ```python
       code = "..."
       import os
       os.makedirs('src/terraform', exist_ok=True)
       with open('src/terraform/main.tf', 'w', encoding='utf-8') as f:
           f.write(code)
       ```
    3. Hand off back to 'Workbook_Editor' or 'IaC_Architect'.
    Ends your message by naming the next speaker: "Executor, please save this code. After saving, the next speaker is Workbook_Editor."
    """,
    llm_config=llm_config,
)

# Executor: Runs the code.
executor = CleanUserProxyAgent(
    name="Executor",
    system_message="""You are the build server.
    1. Execute provided python blocks to save files.
    2. Working directory: '.'.
    """,
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
    code_execution_config={
        "last_n_messages": 3, 
        "work_dir": ".", 
        "use_docker": False
    },
)

# --- Group Chat Flow ---

# GroupChat management
# Removed Reviewer as per user request to streamline process.
# --- Custom Speaker Selection ---

def custom_speaker_selection_func(last_speaker, groupchat):
    """
    Custom speaker selection logic to enforce strict sequential flow and Executor usage.
    """
    messages = groupchat.messages
    if not messages:
        return iac_architect # Start with Architect (or let strict prompt decide?) matches initiation
    
    last_message = messages[-1]
    last_content = last_message.get("content", "")
    
    # 1. CRITICAL: If message has Python code block and sender is NOT Executor, 
    #    we MUST run it. Go to Executor.
    if "```python" in last_content and last_speaker != executor:
        return executor
        
    # 2. If sender is Executor (meaning code just ran), look for explicit handoff
    if last_speaker == executor:
        if "next speaker is IaC_Architect" in last_content:
            return iac_architect
        if "next speaker is Workbook_Editor" in last_content:
            return workbook_editor
        if "next speaker is IaC_Coder" in last_content:
            return iac_coder
            
    # 3. If sender is Architect, they instruct Editor.
    if last_speaker == iac_architect:
        return workbook_editor
        
    # 4. If sender is Editor:
    #    - If they provided code, rule #1 takes care of it (goes to Executor).
    #    - If text only (rare, but possible if just discussing), strictly go to Coder or back to Architect?
    #      Let's rely on explicit text trigger or default to Coder if ambiguous?
    #      Actually, Editor is instructed to explicit name next speaker.
    #      If Editor says "next speaker is IaC_Coder", let's respect that.
    if last_speaker == workbook_editor:
        if "next speaker is IaC_Coder" in last_content:
            return iac_coder
        if "next speaker is IaC_Architect" in last_content:
            return iac_architect
            
    # 5. If sender is Coder:
    #    - If code, rule #1 takes care (Executor).
    #    - Else, back to Editor.
    if last_speaker == iac_coder:
        return workbook_editor

    # Fallback
    return "auto"

groupchat = autogen.GroupChat(
    agents=[iac_architect, workbook_editor, iac_coder, executor],
    messages=[],
    max_round=100,
    speaker_selection_method=custom_speaker_selection_func,
)

# Strict sequential flow managed by the system messages and speaker selection
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)

# --- Start ---



initial_task = (
    "Indítsd el a 'NetDevOps IaC Workbook' generálást a 0-ról."
    "Generáld le sorban az összes fejezetet az 1.0-tól kezdve a 13.0-ig."
    "Használj külön fájlokat: `docs/chapter_01_git.md`, `docs/chapter_02_devcontainers.md`, stb."
    "Szigorúan kövesd a DETAILED SPECIFICATION-t!"
    "\n\n"
    "--- DETAILED SPECIFICATION ---\n"
    f"{spec_content}"
)

print("Starting IaC Workbook Agents...")
executor.initiate_chat(manager, message=initial_task)
